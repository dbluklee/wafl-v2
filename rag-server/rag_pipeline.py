import os
import logging
import ollama
from sqlalchemy import create_engine, text

from embeddings import BGE_M3_Embeddings
from vector_store import MilvusVectorStore
from document_loader import DocumentLoader

logger = logging.getLogger(__name__)


class RAGPipeline:
    """RAG íŒŒì´í”„ë¼ì¸: ë¬¸ì„œ ì¸ë±ì‹± ë° ê²€ìƒ‰"""

    def __init__(self):
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.embeddings = BGE_M3_Embeddings()
        self.vector_store = MilvusVectorStore(dimension=self.embeddings.dimension)
        self.document_loader = DocumentLoader(chunk_size=1000, chunk_overlap=200)

        # Ollama í´ë¼ì´ì–¸íŠ¸ (ë©”ì¸ LLM)
        main_url = os.getenv("OLLAMA_MAIN_URL", "http://112.148.37.41:1884")
        self.llm_client = ollama.Client(host=main_url)
        self.llm_model = "gemma3:27b-it-q4_K_M"

        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        db_url = os.getenv("DATABASE_URL")
        self.engine = create_engine(db_url)

    async def index_documents(self, store_id: int, category: str = "customer") -> dict:
        """
        íŠ¹ì • ë§¤ì¥ì˜ ë¬¸ì„œë“¤ì„ ì¸ë±ì‹±

        Args:
            store_id: ë§¤ì¥ ID
            category: ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ (customer/owner)

        Returns:
            ì¸ë±ì‹± ê²°ê³¼
        """
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¬¸ì„œ ê²½ë¡œ ì¡°íšŒ
            with self.engine.connect() as conn:
                query = text("""
                    SELECT doc_path FROM rag_documents
                    WHERE store_id = :store_id AND category = :category
                """)
                result = conn.execute(query, {"store_id": store_id, "category": category})
                doc_paths = [row[0] for row in result]

            if not doc_paths:
                return {
                    "status": "error",
                    "message": f"ë§¤ì¥ {store_id}ì˜ {category} ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
                }

            # ê¸°ì¡´ ë²¡í„° ì‚­ì œ
            self.vector_store.delete_by_store(store_id, category)

            # ë¬¸ì„œ ë¡œë“œ ë° ì²­í‚¹
            all_chunks = []
            for doc_path in doc_paths:
                chunks = self.document_loader.load_and_chunk(doc_path)
                all_chunks.extend(chunks)

            if not all_chunks:
                return {
                    "status": "error",
                    "message": "ì²­í‚¹ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
                }

            # ì„ë² ë”© ìƒì„±
            embeddings = self.embeddings.embed_documents(all_chunks)

            # ë²¡í„° ìŠ¤í† ì–´ì— ì‚½ì…
            self.vector_store.insert(
                texts=all_chunks,
                embeddings=embeddings,
                store_id=store_id,
                category=category
            )

            return {
                "status": "success",
                "indexed_documents": len(doc_paths),
                "indexed_chunks": len(all_chunks),
                "store_id": store_id,
                "category": category
            }

        except Exception as e:
            logger.error(f"ì¸ë±ì‹± ì˜¤ë¥˜: {str(e)}")
            return {
                "status": "error",
                "message": str(e)
            }

    async def query(self, query: str, store_id: int, category: str = "customer") -> tuple[str, dict]:
        """
        RAG ì¿¼ë¦¬ ì‹¤í–‰

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            store_id: ë§¤ì¥ ID
            category: ë¬¸ì„œ ì¹´í…Œê³ ë¦¬

        Returns:
            tuple[str, dict]: (LLM ì‘ë‹µ, ë””ë²„ê·¸ ì •ë³´)
        """
        try:
            logger.info("="*80)
            logger.info("ğŸ” [RAG] ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘")
            logger.info("="*80)

            # ì¿¼ë¦¬ ì„ë² ë”©
            query_embedding = self.embeddings.embed_query(query)
            logger.info(f"ğŸ“Š ì¿¼ë¦¬ ì„ë² ë”© ì™„ë£Œ (ì°¨ì›: {len(query_embedding)})")

            # ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (ìƒìœ„ 5ê°œ)
            documents = self.vector_store.search(
                query_embedding=query_embedding,
                store_id=store_id,
                category=category,
                top_k=5
            )

            if not documents:
                logger.warning("âš ï¸ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
                return "ì œê°€ ì˜ ëª¨ë¥´ê² ì–´ìš”. ì£„ì†¡í•˜ì§€ë§Œ ì§ì›ì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”.", {"error": "No documents found"}

            logger.info(f"ğŸ“š ê²€ìƒ‰ëœ ë¬¸ì„œ: {len(documents)}ê°œ")
            for i, doc in enumerate(documents, 1):
                logger.info(f"  [{i}] ìœ ì‚¬ë„: {doc['score']:.4f}")
                logger.info(f"      ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {doc['text'][:100]}...")

            # ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ê´€ë ¨ ì •ë³´ ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬
            if documents[0]['score'] < 0.3:
                logger.warning(f"âš ï¸ ìµœê³  ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤: {documents[0]['score']:.4f}")
                return "ì œê°€ ì˜ ëª¨ë¥´ê² ì–´ìš”. ì£„ì†¡í•˜ì§€ë§Œ ì§ì›ì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”.", {"error": "Low relevance score", "max_score": documents[0]['score']}

            # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context = "\n\n".join([doc["text"] for doc in documents])

            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
            prompt = f"""ë‹¹ì‹ ì€ ë§¤ì¥ì˜ ì¹œì ˆí•œ ì§ì›ì…ë‹ˆë‹¤.
ì•„ë˜ ë§¤ì¥ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì†ë‹˜ì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ë‹µë³€ ê·œì¹™:
1. 50ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
2. ì†ë‹˜ì´ ì›í•˜ëŠ” í•µì‹¬ë§Œ ì „ë‹¬í•˜ì„¸ìš”
3. ì¹œì ˆí•˜ì§€ë§Œ ìš”ì ë§Œ ë§í•˜ì„¸ìš”
4. ë¶ˆí•„ìš”í•œ ì„¤ëª…ì€ ìƒëµí•˜ì„¸ìš”
5. **ì¤‘ìš”**: ë¬¸ì„œì— ì •ë³´ê°€ ì—†ê±°ë‚˜ í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ ì ˆëŒ€ ê±°ì§“ë§í•˜ì§€ ë§ˆì„¸ìš”
6. ëª¨ë¥´ëŠ” ë‚´ìš©ì€ "ì œê°€ ì˜ ëª¨ë¥´ê² ì–´ìš”. ì£„ì†¡í•˜ì§€ë§Œ ì§ì›ì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”

ë§¤ì¥ ë¬¸ì„œ:
{context}

ì†ë‹˜ ì§ˆë¬¸: {query}

ì§ì› ë‹µë³€:"""

            logger.info("="*80)
            logger.info("ğŸ¤– [LLM] ì‘ë‹µ ìƒì„±")
            logger.info("="*80)
            logger.info(f"ğŸ“ ìµœì¢… í”„ë¡¬í”„íŠ¸ (ê¸¸ì´: {len(prompt)} ë¬¸ì):")
            logger.info(f"\n{prompt}\n")

            # LLM ì‘ë‹µ ìƒì„±
            response = self.llm_client.generate(
                model=self.llm_model,
                prompt=prompt
            )

            answer = response['response'].strip()

            # 50ì ì œí•œ ì²´í¬ ë° ì¶”ê°€ ì„¤ëª… ì œì•ˆ
            if len(answer) > 50:
                answer = answer[:50] + "..."
                answer += "\n\në” ìì„¸íˆ ì„¤ëª…í•´ë“œë¦´ê¹Œìš”?"

            logger.info(f"ğŸ’¬ LLM ì‘ë‹µ:\n{answer}")
            logger.info("="*80)

            debug_info = {
                "retrieved_documents": [
                    {
                        "score": doc["score"],
                        "text_preview": doc["text"][:200]
                    }
                    for doc in documents
                ],
                "context_length": len(context),
                "final_prompt": prompt,
                "llm_model": self.llm_model,
                "llm_response": answer
            }

            return answer, debug_info

        except Exception as e:
            logger.error(f"ì¿¼ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", {"error": str(e)}
